{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Evaluate answers comparing with ground truth\n",
    "In this notebook:\n",
    "- Import libraries, load configuration variables and create clients\n",
    "- Test the end-to-end process with one query and with queries and expected answers in an Excel file:\n",
    "    + Hybrid search with Semantic ranker\n",
    "    + Filter the chunks leaving the most relevant compared with the user's question\n",
    "    + Generate the answer for the query using the most relevante chunks as the context\n",
    "    + Evaluate with AI Foundry SDK evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install azure-ai-evaluation\n",
    "#%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.evaluation import QAEvaluator\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from common_utils import *\n",
    "\n",
    "# Load Azure OpenAI and AI Search variables and create clients\n",
    "openai_config, ai_search_config = load_config()\n",
    "\n",
    "# Prepare AI Search client\n",
    "# We will use the 'docs' index for this example\n",
    "ai_search_client = SearchClient(endpoint=ai_search_config[\"ai_search_endpoint\"],\n",
    "                                index_name=ai_search_config[\"ai_search_index_name_docs\"],\n",
    "                                credential=AzureKeyCredential(ai_search_config[\"ai_search_apikey\"]))\n",
    "\n",
    "# Initialzing Groundedness and Similarity evaluators\n",
    "model_config = {\n",
    "    \"azure_endpoint\": openai_config[\"aoai_endpoint\"],\n",
    "    \"api_key\": openai_config[\"aoai_key\"],\n",
    "    \"azure_deployment\": openai_config[\"aoai_rerank_model\"],\n",
    "    \"api_version\": openai_config[\"api_version\"]\n",
    "}\n",
    "qa_eval = QAEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with one question the end-to-end process:\n",
    "- Generate query for AI Search\n",
    "- Hybrid search with Semantic ranker\n",
    "- Filter chunks comparing with the question\n",
    "- Generate the answer with the relevant chunks as context\n",
    "- Evaluate the answer compared with the expected answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with one question\n",
    "question = \"What is included in my Northwind Health Plus plan?\"\n",
    "\n",
    "# Hybrid search\n",
    "results, num_results = semantic_hybrid_search(ai_search_client=ai_search_client,\n",
    "                                              openai_client=openai_config[\"openai_client\"],\n",
    "                                              aoai_embedding_model=openai_config[\"aoai_embedding_model\"],\n",
    "                                              query=question,\n",
    "                                              max_docs=10)\n",
    "print(f\"num results: {num_results}\")\n",
    "show_results(results, question)\n",
    "\n",
    "# Valid chunks for the user question\n",
    "valid_chunks, num_chunks = get_filtered_chunks(openai_config[\"openai_client\"],\n",
    "                                               openai_config[\"aoai_rerank_model\"],\n",
    "                                               results, question)\n",
    "\n",
    "# Generate answer:\n",
    "answer = generate_answer(openai_config[\"openai_client\"],\n",
    "                         openai_config[\"aoai_deployment_name\"],\n",
    "                         valid_chunks, question)\n",
    "print(f\"\\n>> Answer:\\n{answer}\")\n",
    "\n",
    "# Evaluate answer\n",
    "expected_answer = \"\"\"The Northwind Health Plus plan includes the following benefits coverage:\n",
    "- Deductible: $2,000 per year.\n",
    "- Coinsurance: 20% of the cost of a covered service after the deductible is met.\n",
    "- Out-of-Pocket Maximum: $4,000 per year, including deductible, coinsurance, and copayments.\n",
    "- In-Network Provider: Lower copayments and coinsurance amounts.\n",
    "- Out-of-Network Provider: Higher copayments and coinsurance amounts.\n",
    "- Preventive Care: Covered at 100% with no copayment, deductible, or coinsurance.\n",
    "- Prescription Drugs: Subject to a copayment, varying by drug type. Generic drugs usually have lower copayments.\n",
    "- Mental Health and Substance Abuse Services: Subject to a copayment and deductible, varying by service type.\"\"\"\n",
    "\n",
    "qa_score = evaluate_answer(qa_eval, question, valid_chunks, answer, expected_answer)\n",
    "print(f'Evaluation results:\\n{qa_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with all questions in an excel file\n",
    "input_file = \"ground_truth.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "data_dict = df.to_dict(orient='records')\n",
    "\n",
    "# For earch line in the input Excel file\n",
    "for i, line in enumerate(data_dict):\n",
    "    question = line['QUESTION']\n",
    "    print(f\"[{i+1}] question: {question}\")\n",
    "\n",
    "    # Hybrid search\n",
    "    results, num_results = semantic_hybrid_search(ai_search_client=ai_search_client,\n",
    "                                                  openai_client=openai_config[\"openai_client\"],\n",
    "                                                  aoai_embedding_model=openai_config[\"aoai_embedding_model\"],\n",
    "                                                  query=question,\n",
    "                                                  max_docs=10)\n",
    "    print(f\"\\tnum results: {num_results}\")\n",
    "    #show_results(results, query)\n",
    "\n",
    "    # Valid chunks for the user question\n",
    "    valid_chunks, num_chunks = get_filtered_chunks(openai_config[\"openai_client\"],\n",
    "                                                openai_config[\"aoai_rerank_model\"],\n",
    "                                                results, question)\n",
    "    print(f\"\\tnum valid chunks: {num_chunks}\")\n",
    "\n",
    "    # Generate answer:\n",
    "    answer = generate_answer(openai_config[\"openai_client\"],\n",
    "                            openai_config[\"aoai_deployment_name\"],\n",
    "                            valid_chunks, question)\n",
    "    print(f\"\\n>> Answer:\\n{answer}\")\n",
    "\n",
    "    # Evaluate answer\n",
    "    expected_answer = line['EXPECTED ANSWER']\n",
    "    qa_score = evaluate_answer(qa_eval, question, valid_chunks, answer, expected_answer)\n",
    "    print(f'Evaluation results:\\n{qa_score}')\n",
    "    print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate answers with conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read test data from Excel file\n",
    "input_file = \"../5_evaluation/ground_truth.xlsx\"\n",
    "df = pd.read_excel(input_file,)\n",
    "data_dict = df.to_dict(orient='records')\n",
    "\n",
    "question = ''\n",
    "history=[]\n",
    "for i, line in enumerate(data_dict):\n",
    "\n",
    "    question = line['QUESTION']\n",
    "\n",
    "    print(f'[{i+1}] Question: {question}')\n",
    "    query = generate_search_query(openai_config[\"openai_client\"],\n",
    "                           openai_config[\"aoai_deployment_name\"],\n",
    "                           question,\n",
    "                           history)\n",
    "    print(f'Rewritten Question: {query}')\n",
    "\n",
    "    # Hybrid search with Semantic ranker\n",
    "    results, num_results = semantic_hybrid_search(ai_search_client=ai_search_client,\n",
    "                                                  openai_client=openai_config[\"openai_client\"],\n",
    "                                                  aoai_embedding_model=openai_config[\"aoai_embedding_model\"],\n",
    "                                                  query=query,\n",
    "                                                  max_docs=50)\n",
    "    print(f\"num results: {num_results}\")\n",
    "    #show_results(results, query)\n",
    "\n",
    "    # Filter valid chunks for the user question\n",
    "    valid_chunks, num_chunks = get_filtered_chunks(openai_config[\"openai_client\"],\n",
    "                                                openai_config[\"aoai_rerank_model\"],\n",
    "                                                results, question)\n",
    "    # Generate answer with best chunks as context and the conversation history:\n",
    "    answer = generate_answer_with_history(openai_config[\"openai_client\"],\n",
    "                                          openai_config[\"aoai_deployment_name\"],\n",
    "                                          valid_chunks,\n",
    "                                          question,\n",
    "                                          history)\n",
    "    print(f\"\\n>> Answer: {answer}\\n\")\n",
    "\n",
    "    # check if the number of question and answer pair has reached the limit of 3 and remove the oldest one\n",
    "    if len(history) >= 3:\n",
    "        history.pop(0)\n",
    "    history.append({\"question\": question, \"answer\": answer})\n",
    "    print(f\"\\nhistory: {json.dumps(history, indent=2)}\\n\")\n",
    "\n",
    "\n",
    "    # Evaluate answer\n",
    "    expected_answer = line['EXPECTED ANSWER']\n",
    "    qa_score = evaluate_answer(qa_eval, question, valid_chunks, answer, expected_answer)\n",
    "    print(f'Evaluation results:\\n{qa_score}')\n",
    "    print('--------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
