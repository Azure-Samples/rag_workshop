{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Indexing\n",
    "In this notebook:\n",
    "- Import libraries, load configuration variables and create clients\n",
    "- Indexing functions: create the index(es), chunk rows/documents and index chunks\n",
    "- Index data from a database: retrieve data from a Database using an endpoint and sql query, chunk the content and index the chunks\n",
    "- Convert PDF files to markdown, chunk and index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries, load configuration variables and create clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install azure-ai-documentintelligence\n",
    "#%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import DocumentContentFormat\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "sys.path.append('..')\n",
    "from common_utils import *\n",
    "\n",
    "# Load Azure OpenAI and AI Search variables and create clients\n",
    "openai_config, ai_search_config = load_config()\n",
    "\n",
    "# Load Document Intelligence configuration\n",
    "doc_intel_endpoint = os.getenv(\"DOC_INTEL_ENDPOINT\")\n",
    "doc_intel_key = os.getenv(\"DOC_INTEL_KEY\")\n",
    "doc_intel_client = DocumentIntelligenceClient(endpoint=doc_intel_endpoint, credential=AzureKeyCredential(doc_intel_key))\n",
    "print(f'doc_intel_endpoint: {doc_intel_endpoint}')\n",
    "\n",
    "# Load SQLite endpoint (run server with 'python app.py')\n",
    "sqlite_endpoint = os.environ[\"SQLITE_ENDPOINT\"]\n",
    "sqlite_user = os.environ[\"SQLITE_USER\"]\n",
    "sqlite_password = os.environ[\"SQLITE_PASSWORD\"]\n",
    "print(f'sqlite_endpoint: {sqlite_endpoint}')\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "OVERLAP_TOKENS = 128 # 25% of 512 tokens is 128 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing functions\n",
    "Personalization and details:\n",
    "- **create_index:** specify your keyword fields and your embeddings fields\n",
    "- **index_documents:** the parameter 'content' is a list in json format with the fields defined when creating the index, converting the data from your source to that json list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AI Search index\n",
    "def create_index(index_name):\n",
    "    # Create an Azure AI Search index client\n",
    "    index_client = SearchIndexClient(endpoint=ai_search_config[\"ai_search_endpoint\"], credential=ai_search_config[\"ai_search_credential\"])\n",
    "    \n",
    "    # Fields definition\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String), #analyzer=\"es.microsoft\"),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String), #analyzer=\"es.microsoft\"),\n",
    "        SearchField(name=\"embeddingTitle\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, vector_search_dimensions=EMBEDDINGS_DIMENSIONS, vector_search_profile_name=\"myHnswProfile\"),\n",
    "        SearchField(name=\"embeddingContent\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, vector_search_dimensions=EMBEDDINGS_DIMENSIONS, vector_search_profile_name=\"myHnswProfile\")\n",
    "    ]\n",
    "\n",
    "    # Configure the vector search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Semantic ranker configuration\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"semantic-config\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            content_fields=[SemanticField(field_name=\"content\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index with the semantic settings\n",
    "    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search)\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(f\"Index '{result.name}' created\")\n",
    "\n",
    "# Chunking Fixed tokens with LangChain\n",
    "def chunk_text(title, text):\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=MAX_TOKENS,\n",
    "        chunk_overlap=OVERLAP_TOKENS\n",
    "        )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    data = []\n",
    "    for chunk in chunks:\n",
    "        row = {'title': title, 'content': chunk}\n",
    "        data.append(row)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Index documents in the Azure AI Search index\n",
    "# Index the batch in Azure AI Search index\n",
    "def index_lote(batch_client, lote, i):\n",
    "    try:\n",
    "        print(f'Indexing until document {i}...')\n",
    "        batch_client.upload_documents(documents=lote)\n",
    "        print('Waiting 15 seconds...')\n",
    "        time.sleep(15)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "# Index the contents or chunks\n",
    "def index_documents(ai_search_endpoint, ai_search_credential, index_name, embedding_client, embedding_model_name, contents):\n",
    "\n",
    "    # Create an index batch client\n",
    "    batch_client = SearchIndexingBufferedSender(\n",
    "                endpoint=ai_search_endpoint,\n",
    "                index_name=index_name,\n",
    "                credential=ai_search_credential\n",
    "            )\n",
    "\n",
    "    lote = []\n",
    "    for i, content in enumerate(contents):  # Index the chunks using the file name as title\n",
    "        #print('=================================================================')\n",
    "        title = content['title']\n",
    "        content = content['content']\n",
    "        print(f\"[{i + 1}]: title: {title}\")\n",
    "        #print(f\"\\t[{content}]\")\n",
    "        document = {\n",
    "            \"id\": str(i),\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            # Create embeddings with ADA-2\n",
    "            \"embeddingTitle\": embedding_client.embeddings.create(input=cut_max_tokens(title), model=embedding_model_name).data[0].embedding,\n",
    "            \"embeddingContent\": embedding_client.embeddings.create(input=cut_max_tokens(content), model=embedding_model_name).data[0].embedding,\n",
    "        }\n",
    "        # Add the document to the batch\n",
    "        lote.append(document)\n",
    "        # Index every 10 documents in the batch\n",
    "        if (i + 1) % 10 == 0:\n",
    "            # Upload documents\n",
    "            print(f'INDEXING BATCH {i + 1}')\n",
    "            index_lote(batch_client, lote, i)\n",
    "            lote = []\n",
    "\n",
    "    # Index the rest of documents after the last batch\n",
    "    if len(lote) > 0:\n",
    "        index_lote(batch_client, lote, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index data from a database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query in a database by endpoint\n",
    "- Requirements: pip install flask\n",
    "- Before sending a query to SQLite install Flask with 'pip install Flask' and run the following command: ***python app.py***\n",
    "\n",
    "Customization the sample:\n",
    "- **query_sqlite_endpoint:**: it sends the SQL query to the database using the endpoint through the Flask web server. Copy and modify it, substituting the parameters to your data source using the REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sqlite_endpoint(sqlite_endpoint, sql, user, password):\n",
    "    # Define the headers and payload\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    payload = {\n",
    "        'query': sql,\n",
    "        'user': user,\n",
    "        'password': password\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.post(sqlite_endpoint, json=payload, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Error executing the query. Status code:\", response.status_code)\n",
    "        print(\"Response:\", response.text)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the AI Search index\n",
    "- create the index\n",
    "- get the data querying the database\n",
    "- chunk the content\n",
    "- index the data\n",
    "- test a query in AI Search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index\n",
    "create_index(ai_search_config[\"ai_search_index_name_regs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query\n",
    "sql = \"\"\"SELECT p.Name, d.Description\n",
    "FROM Product AS p\n",
    "JOIN ProductDescription AS d\n",
    "ON p.ProductID = d.ProductDescriptionID\n",
    "\"\"\"\n",
    "\n",
    "response = query_sqlite_endpoint(sqlite_endpoint, sql, sqlite_user, sqlite_password)\n",
    "\n",
    "if response != None:\n",
    "    # Prepare the data in json where the first field is the title and the second is the content\n",
    "    rows = [None] * len(response)\n",
    "    # Prepare the data in json where the first field is the title and the second is the content\n",
    "    for i, row in enumerate(response):\n",
    "        rows[i] = {\n",
    "            'title': row[0],\n",
    "            'content': row[1]\n",
    "        }\n",
    "    print(json.dumps(rows, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the values of field 'content'\n",
    "chunks = []\n",
    "for i, row in enumerate(rows):\n",
    "    # Create chunks\n",
    "    chunks += chunk_text(row['title'], row['content'])\n",
    "print(f'Number of chunks: {len(chunks)}')\n",
    "print(f'Chunks: {json.dumps(chunks, indent=2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index content retrieved from the database (NO CHUNKING)\n",
    "#index_documents(ai_search_config[\"ai_search_endpoint\"],\n",
    "#                ai_search_config[\"ai_search_credential\"],\n",
    "#                ai_search_config[\"ai_search_index_name_regs\"],\n",
    "#                openai_config[\"openai_client\"],\n",
    "#                openai_config[\"aoai_embedding_model\"],\n",
    "#                rows)\n",
    "\n",
    "# Index content retrieved from the database (CHUNKING)\n",
    "index_documents(ai_search_config[\"ai_search_endpoint\"],\n",
    "                ai_search_config[\"ai_search_credential\"],\n",
    "                ai_search_config[\"ai_search_index_name_regs\"],\n",
    "                openai_config[\"openai_client\"],\n",
    "                openai_config[\"aoai_embedding_model\"],\n",
    "                chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a query\n",
    "query = \"pantalones cortos de hombre\"\n",
    "results, num_results = semantic_hybrid_search(ai_search_config[\"ai_search_client_regs\"],\n",
    "                                              openai_config[\"openai_client\"],\n",
    "                                              openai_config[\"aoai_embedding_model\"],\n",
    "                                              query=query, max_docs=10)\n",
    "show_results(results, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index content from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to convert documents to markdown, chunk and indexing the chunks\n",
    "- process_file: convert every PDF in a folder to markdown with Document Intelligence\n",
    "- chunk_and_index_md_files: chunk every markdown file and index the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the documents to mardown format\n",
    "# Process every PDF in a directory\n",
    "def process_files(input_dir, output_dir, extension):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(extension):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            process_file(file_path, output_dir)\n",
    "\n",
    "# Convert one document to MARKDOWN\n",
    "def process_file(file_path, output_dir):\n",
    "    output_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(file_path))[0] + '.md')\n",
    "    \n",
    "    print(f'Converting {file_path} to {output_file_path} in markdown format...')\n",
    "    try:\n",
    "        # Read the temporal file\n",
    "        with open(file_path, \"rb\") as pdf_file:\n",
    "            pdf_content = pdf_file.read()\n",
    "\n",
    "        # Convert to markdown with Document Intelligence\n",
    "        poller = doc_intel_client.begin_analyze_document(\"prebuilt-layout\",\n",
    "                                                        body=pdf_content,\n",
    "                                                        output_content_format=DocumentContentFormat.MARKDOWN,\n",
    "                                                        content_type=\"application/octet-stream\")\n",
    "        result = poller.result()\n",
    "        markdown = result['content']\n",
    "\n",
    "        # Save the markdown to disk\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown)\n",
    "        print(f\"\\tSaved file [{output_file_path}]\")\n",
    "\n",
    "    except Exception as ex:\n",
    "        markdown = None\n",
    "        print(ex)\n",
    "\n",
    "    return markdown\n",
    "\n",
    "# Chunk and index the markdown files\n",
    "def chunk_and_index_md_files(input_dir):\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.md'):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            print(f'Chunking {file_path} -----------------------------')\n",
    "            # Read the md file\n",
    "            with open(file_path, \"r\", encoding='utf-8') as pdf_file:\n",
    "                text = pdf_file.read()\n",
    "            chunks = chunk_text(filename, text)\n",
    "\n",
    "            # Index the chunk\n",
    "            index_documents(ai_search_config[\"ai_search_endpoint\"],\n",
    "                            ai_search_config[\"ai_search_credential\"], ai_search_config[\"ai_search_index_name_docs\"],\n",
    "                            openai_config[\"openai_client\"],\n",
    "                            openai_config[\"aoai_embedding_model\"],\n",
    "                            chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index\n",
    "create_index(ai_search_config[\"ai_search_index_name_docs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PDF files to markdown\n",
    "process_files('docs', 'docs/markdown', '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk and index the markdown files\n",
    "chunk_and_index_md_files('docs/markdown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a query\n",
    "query = \"healthcare plan\"\n",
    "results, num_results = semantic_hybrid_search(ai_search_config[\"ai_search_client_docs\"],\n",
    "                                              openai_config[\"openai_client\"],\n",
    "                                              openai_config[\"aoai_embedding_model\"],\n",
    "                                              query=query, max_docs=10)\n",
    "show_results(results, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
