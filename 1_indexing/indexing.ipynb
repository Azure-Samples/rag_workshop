{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Indexing\n",
    "In this notebook:\n",
    "- Import libraries, load configuration variables and create clients\n",
    "- Indexing functions: create the index(es), chunk rows/documents and index chunks\n",
    "- Index data from a database: retrieve data from a Database using an endpoint and sql query, chunk the content and index the chunks\n",
    "- Convert PDF files to markdown, chunk and index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries, load configuration variables and create clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install azure-ai-documentintelligence\n",
    "#%pip install langchain\n",
    "#%pip install python-dotenv\n",
    "#%pip install tiktoken\n",
    "#%pip install openai\n",
    "#%pip install azure-search-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommon_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load Azure OpenAI and AI Search variables and create clients\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m openai_config, ai_search_config = \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load Document Intelligence configuration\u001b[39;00m\n\u001b[32m     20\u001b[39m doc_intel_endpoint = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mDOC_INTEL_ENDPOINT\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Angel\\AI GBB\\rag_workshop\\common_utils.py:44\u001b[39m, in \u001b[36mload_config\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     34\u001b[39m aoai_key = os.getenv(\u001b[33m'\u001b[39m\u001b[33mAZURE_OPENAI_API_KEY\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     35\u001b[39m api_version = os.getenv(\u001b[33m'\u001b[39m\u001b[33mAZURE_OPENAI_API_VERSION\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     36\u001b[39m openai_config = {\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maoai_endpoint\u001b[39m\u001b[33m\"\u001b[39m: aoai_endpoint,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maoai_key\u001b[39m\u001b[33m\"\u001b[39m: aoai_key,\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maoai_deployment_name\u001b[39m\u001b[33m\"\u001b[39m: os.getenv(\u001b[33m'\u001b[39m\u001b[33mAZURE_OPENAI_DEPLOYMENT_NAME\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maoai_embedding_model\u001b[39m\u001b[33m\"\u001b[39m: os.getenv(\u001b[33m'\u001b[39m\u001b[33mAZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maoai_rerank_model\u001b[39m\u001b[33m\"\u001b[39m: os.getenv(\u001b[33m'\u001b[39m\u001b[33mAZURE_OPENAI_RERANK_DEPLOYMENT_NAME\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mapi_version\u001b[39m\u001b[33m\"\u001b[39m: api_version,\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Initialize Azure OpenAI client\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mopenai_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mAzureOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mazure_endpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43maoai_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43maoai_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     47\u001b[39m }\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33maoai_endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopenai_config[\u001b[33m\"\u001b[39m\u001b[33maoai_endpoint\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33maoai_deployment_name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopenai_config[\u001b[33m\"\u001b[39m\u001b[33maoai_deployment_name\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Angel\\AI GBB\\rag_workshop\\.venv\\Lib\\site-packages\\openai\\lib\\azure.py:207\u001b[39m, in \u001b[36mAzureOpenAI.__init__\u001b[39m\u001b[34m(self, api_version, azure_endpoint, azure_deployment, api_key, azure_ad_token, azure_ad_token_provider, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;66;03m# define a sentinel value to avoid any typing issues\u001b[39;00m\n\u001b[32m    205\u001b[39m     api_key = API_KEY_SENTINEL\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28mself\u001b[39m._api_version = api_version\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m._azure_ad_token = azure_ad_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Angel\\AI GBB\\rag_workshop\\.venv\\Lib\\site-packages\\openai\\_client.py:122\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    120\u001b[39m     base_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m._default_stream_cls = Stream\n\u001b[32m    135\u001b[39m \u001b[38;5;28mself\u001b[39m.completions = resources.Completions(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Angel\\AI GBB\\rag_workshop\\.venv\\Lib\\site-packages\\openai\\_base_client.py:825\u001b[39m, in \u001b[36mSyncAPIClient.__init__\u001b[39m\u001b[34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[39m\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    809\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    810\u001b[39m     )\n\u001b[32m    812\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    813\u001b[39m     version=version,\n\u001b[32m    814\u001b[39m     limits=limits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     _strict_response_validation=_strict_response_validation,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28mself\u001b[39m._client = http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Angel\\AI GBB\\rag_workshop\\.venv\\Lib\\site-packages\\openai\\_base_client.py:723\u001b[39m, in \u001b[36m_DefaultHttpxClient.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mlimits\u001b[39m\u001b[33m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[32m    722\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mfollow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Client.__init__() got an unexpected keyword argument 'proxies'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import DocumentContentFormat\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from common_utils import *\n",
    "\n",
    "# Load Azure OpenAI and AI Search variables and create clients\n",
    "openai_config, ai_search_config = load_config()\n",
    "\n",
    "# Load Document Intelligence configuration\n",
    "doc_intel_endpoint = os.getenv(\"DOC_INTEL_ENDPOINT\")\n",
    "doc_intel_key = os.getenv(\"DOC_INTEL_KEY\")\n",
    "doc_intel_client = DocumentIntelligenceClient(endpoint=doc_intel_endpoint, credential=AzureKeyCredential(doc_intel_key))\n",
    "print(f'doc_intel_endpoint: {doc_intel_endpoint}')\n",
    "\n",
    "# Load SQLite endpoint (run server with 'python app.py')\n",
    "sqlite_endpoint = os.environ[\"SQLITE_ENDPOINT\"]\n",
    "sqlite_user = os.environ[\"SQLITE_USER\"]\n",
    "sqlite_password = os.environ[\"SQLITE_PASSWORD\"]\n",
    "print(f'sqlite_endpoint: {sqlite_endpoint}')\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "OVERLAP_TOKENS = 128 # 25% of 512 tokens is 128 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing functions\n",
    "Personalization and details:\n",
    "- **create_index:** specify your keyword fields and your embeddings fields\n",
    "- **index_documents:** the parameter 'content' is a list in json format with the fields defined when creating the index, converting the data from your source to that json list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AI Search index\n",
    "def create_index(index_name):\n",
    "    # Create an Azure AI Search index client\n",
    "    index_client = SearchIndexClient(endpoint=ai_search_config[\"ai_search_endpoint\"], credential=ai_search_config[\"ai_search_credential\"])\n",
    "    \n",
    "    # Fields definition\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String), #analyzer=\"es.microsoft\"),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String), #analyzer=\"es.microsoft\"),\n",
    "        SearchField(name=\"embeddingTitle\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, vector_search_dimensions=EMBEDDINGS_DIMENSIONS, vector_search_profile_name=\"myHnswProfile\"),\n",
    "        SearchField(name=\"embeddingContent\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, vector_search_dimensions=EMBEDDINGS_DIMENSIONS, vector_search_profile_name=\"myHnswProfile\")\n",
    "    ]\n",
    "\n",
    "    # Configure the vector search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Semantic ranker configuration\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"semantic-config\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            content_fields=[SemanticField(field_name=\"content\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index with the semantic settings\n",
    "    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search)\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(f\"Index '{result.name}' created\")\n",
    "\n",
    "# Chunking Fixed tokens with LangChain\n",
    "def chunk_text(title, text):\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=MAX_TOKENS,\n",
    "        chunk_overlap=OVERLAP_TOKENS\n",
    "        )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    data = []\n",
    "    for chunk in chunks:\n",
    "        row = {'title': title, 'content': chunk}\n",
    "        data.append(row)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Index documents in the Azure AI Search index\n",
    "# Index the batch in Azure AI Search index\n",
    "def index_lote(batch_client, lote, i):\n",
    "    try:\n",
    "        print(f'Indexing until document {i}...')\n",
    "        batch_client.upload_documents(documents=lote)\n",
    "        print('Waiting 15 seconds...')\n",
    "        time.sleep(15)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "# Index the contents or chunks\n",
    "def index_documents(ai_search_endpoint, ai_search_credential, index_name, embedding_client, embedding_model_name, contents):\n",
    "\n",
    "    # Create an index batch client\n",
    "    batch_client = SearchIndexingBufferedSender(\n",
    "                endpoint=ai_search_endpoint,\n",
    "                index_name=index_name,\n",
    "                credential=ai_search_credential\n",
    "            )\n",
    "\n",
    "    lote = []\n",
    "    for i, content in enumerate(contents):  # Index the chunks using the file name as title\n",
    "        #print('=================================================================')\n",
    "        title = content['title']\n",
    "        content = content['content']\n",
    "        print(f\"[{i + 1}]: title: {title}\")\n",
    "        #print(f\"\\t[{content}]\")\n",
    "        document = {\n",
    "            \"id\": str(i),\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            # Create embeddings with ADA-2\n",
    "            \"embeddingTitle\": embedding_client.embeddings.create(input=cut_max_tokens(title), model=embedding_model_name).data[0].embedding,\n",
    "            \"embeddingContent\": embedding_client.embeddings.create(input=cut_max_tokens(content), model=embedding_model_name).data[0].embedding,\n",
    "        }\n",
    "        # Add the document to the batch\n",
    "        lote.append(document)\n",
    "        # Index every 10 documents in the batch\n",
    "        if (i + 1) % 10 == 0:\n",
    "            # Upload documents\n",
    "            print(f'INDEXING BATCH {i + 1}')\n",
    "            index_lote(batch_client, lote, i)\n",
    "            lote = []\n",
    "\n",
    "    # Index the rest of documents after the last batch\n",
    "    if len(lote) > 0:\n",
    "        index_lote(batch_client, lote, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index data from a database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query in a database by endpoint\n",
    "- Requirements: pip install flask\n",
    "- Before sending a query to SQLite install Flask with 'pip install Flask' and run the following command: ***python app.py***\n",
    "\n",
    "Customization the sample:\n",
    "- **query_sqlite_endpoint:**: it sends the SQL query to the database using the endpoint through the Flask web server. Copy and modify it, substituting the parameters to your data source using the REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sqlite_endpoint(sqlite_endpoint, sql, user, password):\n",
    "    # Define the headers and payload\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    payload = {\n",
    "        'query': sql,\n",
    "        'user': user,\n",
    "        'password': password\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.post(sqlite_endpoint, json=payload, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Error executing the query. Status code:\", response.status_code)\n",
    "        print(\"Response:\", response.text)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the AI Search index\n",
    "- create the index\n",
    "- get the data querying the database\n",
    "- chunk the content\n",
    "- index the data\n",
    "- test a query in AI Search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index\n",
    "create_index(ai_search_config[\"ai_search_index_name_regs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query\n",
    "sql = \"\"\"SELECT p.Name, d.Description\n",
    "FROM Product AS p\n",
    "JOIN ProductDescription AS d\n",
    "ON p.ProductID = d.ProductDescriptionID\n",
    "\"\"\"\n",
    "\n",
    "response = query_sqlite_endpoint(sqlite_endpoint, sql, sqlite_user, sqlite_password)\n",
    "\n",
    "if response != None:\n",
    "    # Prepare the data in json where the first field is the title and the second is the content\n",
    "    rows = [None] * len(response)\n",
    "    # Prepare the data in json where the first field is the title and the second is the content\n",
    "    for i, row in enumerate(response):\n",
    "        rows[i] = {\n",
    "            'title': row[0],\n",
    "            'content': row[1]\n",
    "        }\n",
    "    print(json.dumps(rows, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the values of field 'content'\n",
    "chunks = []\n",
    "for i, row in enumerate(rows):\n",
    "    # Create chunks\n",
    "    chunks += chunk_text(row['title'], row['content'])\n",
    "print(f'Number of chunks: {len(chunks)}')\n",
    "print(f'Chunks: {json.dumps(chunks, indent=2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index content retrieved from the database (NO CHUNKING)\n",
    "#index_documents(ai_search_config[\"ai_search_endpoint\"],\n",
    "#                ai_search_config[\"ai_search_credential\"],\n",
    "#                ai_search_config[\"ai_search_index_name_regs\"],\n",
    "#                openai_config[\"openai_client\"],\n",
    "#                openai_config[\"aoai_embedding_model\"],\n",
    "#                rows)\n",
    "\n",
    "# Index content retrieved from the database (CHUNKING)\n",
    "index_documents(ai_search_config[\"ai_search_endpoint\"],\n",
    "                ai_search_config[\"ai_search_credential\"],\n",
    "                ai_search_config[\"ai_search_index_name_regs\"],\n",
    "                openai_config[\"openai_client\"],\n",
    "                openai_config[\"aoai_embedding_model\"],\n",
    "                chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a query\n",
    "query = \"pantalones cortos de hombre\"\n",
    "results, num_results = semantic_hybrid_search(ai_search_config[\"ai_search_client_regs\"],\n",
    "                                              openai_config[\"openai_client\"],\n",
    "                                              openai_config[\"aoai_embedding_model\"],\n",
    "                                              query=query, max_docs=10)\n",
    "show_results(results, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index content from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to convert documents to markdown, chunk and indexing the chunks\n",
    "- process_file: convert every PDF in a folder to markdown with Document Intelligence\n",
    "- chunk_and_index_md_files: chunk every markdown file and index the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the documents to mardown format\n",
    "# Process every PDF in a directory\n",
    "def process_files(input_dir, output_dir, extension):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(extension):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            process_file(file_path, output_dir)\n",
    "\n",
    "# Convert one document to MARKDOWN\n",
    "def process_file(file_path, output_dir):\n",
    "    output_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(file_path))[0] + '.md')\n",
    "    \n",
    "    print(f'Converting {file_path} to {output_file_path} in markdown format...')\n",
    "    try:\n",
    "        # Read the temporal file\n",
    "        with open(file_path, \"rb\") as pdf_file:\n",
    "            pdf_content = pdf_file.read()\n",
    "\n",
    "        # Convert to markdown with Document Intelligence\n",
    "        poller = doc_intel_client.begin_analyze_document(\"prebuilt-layout\",\n",
    "                                                        body=pdf_content,\n",
    "                                                        output_content_format=DocumentContentFormat.MARKDOWN,\n",
    "                                                        content_type=\"application/octet-stream\")\n",
    "        result = poller.result()\n",
    "        markdown = result['content']\n",
    "\n",
    "        # Save the markdown to disk\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown)\n",
    "        print(f\"\\tSaved file [{output_file_path}]\")\n",
    "\n",
    "    except Exception as ex:\n",
    "        markdown = None\n",
    "        print(ex)\n",
    "\n",
    "    return markdown\n",
    "\n",
    "# Chunk and index the markdown files\n",
    "def chunk_and_index_md_files(input_dir):\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.md'):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            print(f'Chunking {file_path} -----------------------------')\n",
    "            # Read the md file\n",
    "            with open(file_path, \"r\", encoding='utf-8') as pdf_file:\n",
    "                text = pdf_file.read()\n",
    "            chunks = chunk_text(filename, text)\n",
    "\n",
    "            # Index the chunk\n",
    "            index_documents(ai_search_config[\"ai_search_endpoint\"],\n",
    "                            ai_search_config[\"ai_search_credential\"], ai_search_config[\"ai_search_index_name_docs\"],\n",
    "                            openai_config[\"openai_client\"],\n",
    "                            openai_config[\"aoai_embedding_model\"],\n",
    "                            chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the AI Search index\n",
    "- create the index\n",
    "- convert PDF files to markdown\n",
    "- chunk and index the chunks\n",
    "- test a query in AI Search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index\n",
    "create_index(ai_search_config[\"ai_search_index_name_docs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PDF files to markdown\n",
    "process_files('docs', 'docs/markdown', '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk and index the markdown files\n",
    "chunk_and_index_md_files('docs/markdown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a query\n",
    "query = \"healthcare plan\"\n",
    "results, num_results = semantic_hybrid_search(ai_search_config[\"ai_search_client_docs\"],\n",
    "                                              openai_config[\"openai_client\"],\n",
    "                                              openai_config[\"aoai_embedding_model\"],\n",
    "                                              query=query, max_docs=10)\n",
    "show_results(results, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
