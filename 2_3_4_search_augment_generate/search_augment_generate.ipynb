{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2, 3 and 4: Search, Augment and Generate the Answer\n",
    "In this notebook there are several parts:\n",
    "- Import libraries, load configuration variables and create clients\n",
    "- Hybrid search with Semantic ranker\n",
    "- Filter the chunks leaving the most relevant compared with the user's question\n",
    "- Generate the answer for the query using the most relevante chunks as the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries, load configuration variables and create clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install python-dotenv\n",
    "#%pip install openai\n",
    "#%pip install tiktoken\n",
    "#%pip install azure-search-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from common_utils import *\n",
    "\n",
    "# Load Azure OpenAI and AI Search variables and create clients\n",
    "openai_config, ai_search_config = load_config()\n",
    "\n",
    "# Prepare AI Search client\n",
    "# We will use the 'docs' index for this example\n",
    "ai_search_client = SearchClient(endpoint=ai_search_config[\"ai_search_endpoint\"],\n",
    "                                index_name=ai_search_config[\"ai_search_index_name_docs\"],\n",
    "                                credential=AzureKeyCredential(ai_search_config[\"ai_search_apikey\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Step by Step\n",
    "1. Search in AI Search\n",
    "2. Filter relevant chunks\n",
    "3. Generate answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Search in AI Search with hybrid (keyword and vector searches) with semantic ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the query for the question\n",
    "question = \"What is included in my Northwind Health Plus plan?\"\n",
    "\n",
    "# Hybrid search\n",
    "results, num_results = semantic_hybrid_search(ai_search_client=ai_search_client,\n",
    "                                              openai_client=openai_config[\"openai_client\"],\n",
    "                                              aoai_embedding_model=openai_config[\"aoai_embedding_model\"],\n",
    "                                              query=question,\n",
    "                                              max_docs=10)\n",
    "print(f\"num results: {num_results}\")\n",
    "print(f\"num len(results): {len(results)}\")\n",
    "show_results(results, question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Filter the chunks compared with the user's question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid chunks for the user question\n",
    "valid_chunks, num_chunks = get_filtered_chunks(openai_config[\"openai_client\"],\n",
    "                                               openai_config[\"aoai_rerank_model\"],\n",
    "                                               results,\n",
    "                                               question)\n",
    "print(f\"num valid chunks: {num_chunks}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Generate the answer using the relevant chunks as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer:\n",
    "answer = generate_answer(openai_config[\"openai_client\"],\n",
    "                         openai_config[\"aoai_deployment_name\"],\n",
    "                                       valid_chunks,\n",
    "                                       question)\n",
    "print(f\"\\n>> Answer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## End-to-end process:\n",
    "\n",
    "question = \"What is included in my Northwind Health Plus plan?\"\n",
    "print(f'Question: {question}')\n",
    "\n",
    "# Hybrid search with Semantic ranker\n",
    "results, num_results = semantic_hybrid_search(ai_search_client=ai_search_client,\n",
    "                                              openai_client=openai_config[\"openai_client\"],\n",
    "                                              aoai_embedding_model=openai_config[\"aoai_embedding_model\"],\n",
    "                                              query=question,\n",
    "                                              max_docs=50)\n",
    "print(f\"num results: {num_results}\")\n",
    "show_results(results, question)\n",
    "\n",
    "# Filter valid chunks for the user question\n",
    "valid_chunks, num_chunks = get_filtered_chunks(openai_config[\"openai_client\"],\n",
    "                                               openai_config[\"aoai_rerank_model\"],\n",
    "                                               results, question)\n",
    "\n",
    "# Generate answer:\n",
    "answer = generate_answer(openai_config[\"openai_client\"],\n",
    "                         openai_config[\"aoai_deployment_name\"],\n",
    "                         valid_chunks, question)\n",
    "print(f\"\\n>> Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End Process Using conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## End-to-end process using conversation history:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read test data from Excel file\n",
    "input_file = \"../5_evaluation/ground_truth.xlsx\"\n",
    "df = pd.read_excel(input_file,)\n",
    "data_dict = df.to_dict(orient='records')\n",
    "\n",
    "question = ''\n",
    "history=[]\n",
    "for i, line in enumerate(data_dict):\n",
    "\n",
    "    question = line['QUESTION']\n",
    "\n",
    "    print(f'[{i+1}] Question: {question}')\n",
    "    query = generate_search_query(openai_config[\"openai_client\"],\n",
    "                           openai_config[\"aoai_deployment_name\"],\n",
    "                           question,\n",
    "                           history)\n",
    "    print(f'Rewritten Question: {query}')\n",
    "\n",
    "    # Hybrid search with Semantic ranker\n",
    "    results, num_results = semantic_hybrid_search(ai_search_client=ai_search_client,\n",
    "                                                  openai_client=openai_config[\"openai_client\"],\n",
    "                                                  aoai_embedding_model=openai_config[\"aoai_embedding_model\"],\n",
    "                                                  query=query,\n",
    "                                                  max_docs=50)\n",
    "    print(f\"num results: {num_results}\")\n",
    "    #show_results(results, query)\n",
    "\n",
    "    # Filter valid chunks for the user question\n",
    "    valid_chunks, num_chunks = get_filtered_chunks(openai_config[\"openai_client\"],\n",
    "                                                openai_config[\"aoai_rerank_model\"],\n",
    "                                                results, question)\n",
    "    # Generate answer with best chunks as context and the conversation history:\n",
    "    answer = generate_answer_with_history(openai_config[\"openai_client\"],\n",
    "                                          openai_config[\"aoai_deployment_name\"],\n",
    "                                          valid_chunks,\n",
    "                                          question,\n",
    "                                          history)\n",
    "    print(f\"\\n>> Answer: {answer}\\n\")\n",
    "\n",
    "    # check if the number of question and answer pair has reached the limit of 3 and remove the oldest one\n",
    "    if len(history) >= 3:\n",
    "        history.pop(0)\n",
    "    history.append({\"question\": question, \"answer\": answer})\n",
    "    print(f\"\\nhistory: {json.dumps(history, indent=2)}\\n\")\n",
    "    print(\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
